[l] Benutzt hier jemand eine Nvidia Titan V fuer numerische Berechnungen? Die scheint eine Art FDIV-Bug zu haben.One engineer told The Register that when he tried to run identical simulations of an interaction between a protein and enzyme on Nvidiaâ€™s Titan V cards, the results varied. After repeated tests on four of the top-of-the-line GPUs, he found two gave numerical errors about 10 per cent of the time.Ich finde ja, die sehen das viel zu negativ. Die Karte hat auch einen Hardware-Zufallszahlengenerator!Update: Ein Einsender hat eine Alternativerklaerung:Erstmal klingt es nur nach jemanden dessen Code auf der neuesten GPU nicht mehr richtig funktioniert.Das kann bei GPUs sehr leicht passieren, weil sie massiv parallel auf den gleichen Problemen arbeiten, das Programmiermodell sehr wenige Garantien darueber gibt, in welcher Reihenfolge der Code abgearbeitet wird und oftmals schlampig synchronisiert wird. Man findet sehr haeufig Sachen im Code, die laut Programmiermodell eigentlich unsafe sind, aber auf bestehender Hardware eben funktionieren, weil dort noch zusaetzliche Randbedingungen bezueglich Ausfuehrungsreihenfolge eingehalten werden. Da wird dann gerne Code geschrieben, der etwas schneller ist, weil er sich Synchronisierungen spart. Bei der Titan V hat NVidia wohl die Verarbeitung von Spruengen verbessert, was aber gleichzeitig dafuer sorgen duerfte, das jetzt ploetzlich Sachen gleichzeitig passieren koennen, die frueher immer in definierter Reihenfolge ausgefuehrt wurden. Zusaetzlich hat die Titan V mehr Kerne als aeltere GPUs, was ebenfalls dazu fuehrt, das mehr Sachen gleichzeitig ausgefuehrt werden, die es frueher nie wurden. Ich sehe da eine hohe Wahrscheinlichkeit, das es sich einfach um kaputten Code handelt, dessen Bugs vorher nicht aufgefallen sind, weil die Eigenschaften der alten Hardware dazu gefuehrt haben, das die Bugs nie getriggert wurden. (Danke, Lothar)

[l] OK, das wird mir jetzt hier gerade zu krass. Besonders niederschmetternd war ein Leserbrief zu Alternativlos 39 aus einem Elektronik-KMU vom Lande, der da die Apokalypse grafisch beschrieben hat, und dann am Schluss meinte, ich solle das bitte nicht ins Blog tun, das koenne man rueckverfolgen. Nachdem er als Sahnehaeubchen erwaehnte, dass die Firma ja jetzt IOT-Geraete bauen will.Daher jetzt auch mal was Konstruktives. Eine Einsendung von Kris Koehntopp, wie man Rollout und Testing richtig macht.Victim Blaming: »Patching is hard? Yes—and every major tech player, no matter how sophisticated they are, has had catastrophic failures when they tried to change something. Google once bricked Chromebooks with an update. A Facebook configuration change took the site offline for 2.5 hours. Microsoft ruined network configuration and partially bricked some computers; even their newest patch isn't trouble-free. An iOS update from Apple bricked some iPad Pros. Even Amazon knocked AWS off the air.«Wo ich arbeite haben wir dieselbe Beobachtung gemacht. Wir haben ein Outage Budget, d.h. wir messen die tatsaechlichen Einnahmen und vergleichen mit den vorhergesagten Einnahmen. Ist durch einen missglueckten Rollout ein Einnahmeausfall zu verzeichnen, buchen wir das vom Outage Budget ab.Wir versuchen das so gut als moeglich zu treffen. Wenn wir mehr Outage als geplant haben, ist das zu viel Risk Taking, wenn wir zu wenig Outage hatten, ist das nicht genug Risk Taking und wir sind zu langsam und complacent.Haeufige Rollouts sind kleine Changes und die sind viel besser zu kontrollieren als grosse unuebersichtliche Changes. Daher versuchen wir, so oft als moeglich einen Rollout (== Patch) zu machen und so die Patches moeglichst klein zu halten.Wenn wir Rollout-Probleme haben, dann meist in den Subsystemen, die wir nicht oft genug ausrollen und in denen dann sekundaere Changes (== Library Changes, die mit sich schneller aendernden Systemen geteilt werden) den Rollout zerknallen. Die Loesung ist fuer uns, so was auch dann auszurollen wenn sich im Code selbst nix geaendert hat (also Dependency-Änderungen auszurollen).Alles in allem ist das eine sehr anschauliche Darstellung von Technical Debt: Je groesser der Diff zwischen ausgerollt und trunk ist, um zu wahrscheinlicher ist es, dass Trunk in Production explodiert.Um Patching sicher zu machen, muss man es oft tun.Wenn man oft patchen moechte, muessen Patches und deren Rollouts ein Operativer Prozess und kein Upgrade-Migrationsprojekt sein.Wenn Rollouts ein operative Prozess sein sollen, dann muss man Testen in der Produktion sicher und ueberlebbar machen.Um Testen in der Produktion sicher zu machen, muss man:das Austeilen von Code und die Aktivierung von Code trennen, also Feature Flags und Experiments einfuehren,sich aendernde persistente Datenstrukturen doppelt fuehren (alte und neue Version gleichzeitig und parallel aktualisieren, sodass alter und neuer Code coexistieren koennen).Reporting und Monitoring exzessiv ausbauen und den Monitoring Lag (Event Timestamp vs. Timestamp in dem das Event im Monitoring auf dem Operator Screen sichtbar wird) mitloggen und Anzeigen ("Monitoring Framerate einblenden")und den Leuten klar machen, dass es wichtig ist, eine Fehlerkultur zu etablieren ("blameless postmortem" einfuehren und durchsetzen).CanariesOvercapacitynur 2 Versionen aktiv zur Zeit (also nicht drei- oder mehrphasige Rollouts laufen haben)Alles kein Hexenwerk eigentlich.Ein Canary ist ein Kanarienvogel im Kohleminen-Sinn. Man schiebt die neue Version nicht gleich auf das ganze Rechenzentrum, sondern erstmal nur auf ein paar Kisten, und da routet man nur ein paar User hin. Und guckt, ob es platzt. Das ist an sich eine gute Praxis, aber man muss aufpassen, dass man das nur kurzzeitig macht. Ich habe schon Firmen gesehen, die praktisch permanent diverse Versionen parallel ausgerollt hatten. Das ist nicht gut.Overcapacity heisst einfach, dass man nicht am Limit faehrt mit seiner Hardware, damit man eine Performance-Regression zur Not mal kurzzeitig abfangen kann.Stoert euch bitte nicht an dem Denglisch, das ist in der Ops-Abteilung von internationalen Firmen durchaus normal :-)Update: Kris hat das auch in sein Blog getan und verlinkt dort auch ein paar Vortraege, die er in dem Bereich gehalten hat.

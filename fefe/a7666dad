[l] Ich habe ja bei meinem IoT-Vortrag vor einer Weile am Ende ein paar Bonusfolien gehabt. Ich denke mir bei Vortraegen immer, OK, die bezahlen mich dafuer, dass ich ihnen sage, was sie hoeren wollen (vielleicht nicht unbedingt von den Schlussfolgerungen, aber vom Thema her), aber ich tue ihnen einen viel groesseren Gefallen, wenn ich ihnen auch sage, was sie hoeren sollten, nicht nur was sie hoeren wollen.Jedenfalls waren meine Bonusfolien u.a. zu Machine Learning, und ich mache da den Punkt, dass man gar nicht versteht, was man eigentlich gerade genau trainiert hat. Ich brachte als Beispiel Adversarial Examples, aber das lag vor allem daran, dass ich kein besseres Beispiel hatte. Das besser Beispiel ist jetzt da: Vice ueber das Anti-Bullying-AI von Google. Die Primaerquelle ist dieser Artikel vom Februar 2017. Money Quote aus dem:However, computer scientists and others on the internet have found the system unable to identify a wide swath of hateful comments, while categorizing innocuous word combinations like “hate is bad” and “garbage truck” as overwhelmingly toxic.Ich moechte mal die steile These wagen, dass das daran liegt, dass Menschen das Problem fuer zu schwierig hielten, um es selber loesen zu koennen, und eine Abkuerzung gesucht haben, bei denen am Ende eine KI Schuld waere, nicht sie. Bei der es am Ende eine (schlechte) Ausrede gibt, wieso das nicht mein Totalversagen ist, was wir hier gerade beobachten, sondern da hat halt, aeh, like, voll die Technik versagt, und so. AI is difficult, let's go shopping!Die Einsicht daran bringt aber der Motherboard-Artikel gut auf den Punkt. Was die ihrem Machine Learning antrainiert haben, weil es naemlich einfacher zu trainieren ist als tatsaechlich boese Aussagen, sind Schimpfwoerter. Money Quote:The tool seems to rank profanity as highly toxic, while deeply harmful statements are often deemed safeBoese Woerter benutzen, das kann die KI (auch nicht wirklich, aber so ansatzweise wenigstens) erkennen. Aber wenn jemand etwas wirklich boeses sagt, etwas, das den Gegenueber moeglicherweise gar in den Selbstmord treibt, so fies und gemein ist es, und es nagt die ganze Zeit am Unterbewusstsein und du kriegst es nicht weg, sowas kann die KI nicht erkennen.Ich moechte an dieser Stelle zu Protokoll geben, dass ich auch nicht moechte, dass eine KI sowas zu erkennen versucht. Denn das menschliche Verhalten in Situationen, wo Software Regeln durchsetzen soll, ist immer dasselbe. Das wird als Herausforderung genommen, nicht als hilfreiche Polizeiarbeit. Das ist genau so "after the fact"-Symptom-Filter-Schlangenoel wie Antiviren.Man kann soziale Probleme nicht technisch loesen.

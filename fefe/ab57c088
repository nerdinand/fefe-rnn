[l] Ich bin seit ein paar Tagen am Ãœberlegen, wie man Logdaten am besten durchsuchbar macht. Fuer normalen Fliesstext habe ich mal eine Volltextsuche geschrieben, und das erfolgreich fuer HTML-Seiten und fuer Software-Quellcode benutzt. Dort macht man das so, dass man eine Datenstruktur baut, die einem sagt: Dieses Wort kommt in diesen Dokumenten vor.Bei Logdaten ist das fuer den Fuss, denn der Index wuerde einem sagen: Dieses Wort kommt in allen Logs des Webservers vor. Ja super. Da bin ich dann so weit wie vorher und muss die alle der Reihe nach durchsuchen.Oder man koennte "Dokument" umdeuten, und zum Beispiel sagen: Alle 15 Minuten Weblog sind ein Dokument.Aber man will ja auch andere Arten von Anfragen stellen koennen bei Logdaten. Man will ja sowas fragen koennen wie: Welche IPs haben in den letzten Stunden mehr als doppelt so haeufig wie der Median zugegriffen?Oder, bei Maillogs: Welche IP, die wir sonst selten sehen, versucht das Absetzen von mehr als 2 Mails in einer Minute.Fuer diese Anfragen von Logs reicht ein Volltextindex nicht.Und eigentlich wil man diese Art von Anfragen auch nicht ad hoc auf den Corpus anwenden, sondern man will sie auf einen vorbeifliessenden Stream ansetzen, und dann einen Trigger ausloesen koennen.Nun kann man da auf mehrere Arten rangehen. Ich ueberlege gerade, ob man das nicht weniger schlimm machen kann, wenn man die Daten spaeter durchsuchen will. Also man laeuft immer noch komplett durch (ich vermute, dass das fuer einige ad-hoc-Anfragen gar nicht anders gehen wird), aber man reduziert den Aufwand, den man treiben muss. Ich dachte mir das so, dass man aus den rohen Logdaten die Felder extrahiert, d.h. die Logdaten in einen pro Logzeilentyp konstanten Format-String (wie bei printf/strftime) und die Datenfelder zerlegt. Und dann speichert man den Stream binaer ab, die Format-Strings jeweils nur als Referenz auf ein Dictionary, und die Inhalte der Felder aus den Daten herausgeparsed. Dann muss man statt 127.0.0.1 nur vier Bytes abspeichern. Pro Logzeile legt man dann eine Referenz auf den Format-String ab (ich nenne das mal Template), dann das Offset zur naechsten Zeile, und dann binaer kodiert die ganzen Werte. Die Idee waere, dass man dann beim Suchen weiss, an welchem Template man interessiert ist, und die anderen schnell ueberspringen kann.Konzeptionell versuche ich also ein Komprimierungsverfahren zu basteln, das ein Matching ohne vollstaendige Rekonstruktion der Quelldaten erlaubt. Oder zumindest ein fail fast beim Matching.Was ich an der Stelle unbedingt vermeiden moechte sind irgendwelche Sammlungen von regulaeren Ausdruecken. Das Parsing muss automatisiert funktionieren.Der erste Schritt dabei waere, dass man erstmal typische Felder erkennt. Ich habe da mal eine Heuristik gehackt, die erkennt schonmal IP-Adressen, Timestamps in einigen ueblichen Formaten, Hostnamen, Dateinamen, Pfadnamen, URLs, Email-Adressen. Das Problem ist halt, dass man Hostnamen und Dateinamen nicht wirklich erkennen und auseinanderhalten kann. Moeglicherweise braucht es diese Aufteilung ja auch gar nicht und ich koennte auch einfach sagen: Das ist ein String-Feld.Nehmen wir mal diese Logzeile hier:Jul 17 12:34:56 ptrace sshd[2342]: Accepted publickey for usereins from 1.2.3.4 port 59932 ssh2: ED25519 00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ffDaraus macht mein Tool sowas wie%t %h %P[%i]: Accepted publickey for usereins from %4 port %i ssh2: ED25519 00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ffDen Hex-String koennte mein Tool auch erkennen, tut es aber im Moment noch nicht. Aber das groessere Problem ist, wenn ich zwei solcher Zeilen habe, eine mit usereins und eine mit userzwei, wie fuehre ich die Templates zusammen und mache aus usereins/userzwei ein %s?Klassische Verfahren fuer sowas sind Edit-Distance/Levenshtein oder sowas, aber das muss doch auch irgendwie geschickter gehen. Und ich braeuchte das auf Woertern, nicht auf Zeichen. Bonusproblem: Mit Feldern klarkommen, die Leerzeichen beinhalten duerfen.Daran knabbere ich gerade herum.Spannenderweise findet man fuer dieses Problem auf Anhieb wenig Kram, weil alles mit Template im Namen in die andere Richtung geht, aus Rohdaten und einem Template den String zu rekonstruieren. Was ich ja trivial genug finde, dass mir nicht klar ist, wieso sich so viele Leute damit beschaeftigen. Aber gut.Vielleicht fehlt mir auch einfach der richtige Suchbegriff?Update: A-Ha! Der erste sachdienliche Hinweis geht ein. Ein verwandtes Problem ist das Longest common subsequence problem.Update: Von Microsoft Research gibt es ein Paper, das genau die Fragestellung zu behandeln scheint. Die arbeiten da mit endlichen Automaten.Update: Und hier ist noch ein Paper von einem Haufen MIT-Leuten. Das scheint mir aber inhaltlich nicht weiterzuhelfen.Update: Ein verwandtes Problem aus der Bioinformatik ist das Sequenzalignment.
